"""
æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
æ¯”è¾ƒä¸åŒLLMæ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬ï¼š
- å“åº”æ—¶é—´
- å“åº”è´¨é‡ 
- å†…å­˜ä½¿ç”¨
- æˆåŠŸç‡
"""

import time
import psutil
import json
import asyncio
import statistics
import sys
import os
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict

# æ·»åŠ ai_serviceè·¯å¾„
sys.path.append(str(Path(__file__).parent / "ai_service"))

try:
    from ai_service.config import get_config
    from ai_service.local_llm_adapter import safe_generate_response
    from ai_service.ai_service import local_llm_generate, set_active_model, get_active_model
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥AIæœåŠ¡æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿AIæœåŠ¡æ­£åœ¨è¿è¡Œæˆ–è€…æ¨¡å—è·¯å¾„æ­£ç¡®")
    sys.exit(1)

@dataclass
class BenchmarkResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    model_name: str
    prompt: str
    response: str
    response_time_ms: float
    memory_usage_mb: float
    success: bool
    error_message: Optional[str] = None
    tokens_per_second: Optional[float] = None
    response_length: int = 0

@dataclass 
class ModelBenchmarkSummary:
    """æ¨¡å‹åŸºå‡†æµ‹è¯•æ±‡æ€»"""
    model_name: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    success_rate: float
    avg_response_time_ms: float
    min_response_time_ms: float
    max_response_time_ms: float
    std_response_time_ms: float
    avg_memory_usage_mb: float
    avg_tokens_per_second: float
    avg_response_length: float
    quality_score: float = 0.0

class ModelBenchmark:
    """æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•å™¨"""
        self.config = get_config()
        self.model_config = self.config.get_model_config()
        self.supported_models = self.model_config.supported_models
        self.results: List[BenchmarkResult] = []
        self.test_prompts = self._get_test_prompts()
        
    def _get_test_prompts(self) -> List[Dict[str, str]]:
        """è·å–æµ‹è¯•æç¤ºè¯é›†åˆ"""
        return [
            {
                "name": "ç®€å•å¯¹è¯",
                "prompt": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
                "category": "conversation"
            },
            {
                "name": "åˆ›æ„å†™ä½œ",
                "prompt": "å†™ä¸€ä¸ªå…³äºå’–å•¡åº—çš„50å­—å°æ•…äº‹ã€‚",
                "category": "creative_writing"
            },
            {
                "name": "æŠ€æœ¯è§£é‡Š",
                "prompt": "ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
                "category": "explanation"
            },
            {
                "name": "é—®é¢˜è§£å†³",
                "prompt": "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿè¯·ç»™å‡º3ä¸ªå…·ä½“å»ºè®®ã€‚",
                "category": "problem_solving"
            },
            {
                "name": "è§’è‰²æ‰®æ¼”",
                "prompt": "å‡è®¾ä½ æ˜¯ä¸€ä¸ªå’–å•¡åº—è€æ¿ï¼Œä¸€ä½é¡¾å®¢æŠ±æ€¨å’–å•¡å¤ªè‹¦ï¼Œä½ ä¼šå¦‚ä½•å›åº”ï¼Ÿ",
                "category": "role_play"
            },
            {
                "name": "é€»è¾‘æ¨ç†",
                "prompt": "å¦‚æœæ‰€æœ‰çš„é¸Ÿéƒ½ä¼šé£ï¼Œä¼é¹…æ˜¯é¸Ÿï¼Œé‚£ä¹ˆä¼é¹…ä¼šé£å—ï¼Ÿè¯·è§£é‡Šä¸ºä»€ä¹ˆã€‚",
                "category": "logic"
            },
            {
                "name": "JSONæ ¼å¼",
                "prompt": "è¯·ä»¥JSONæ ¼å¼æè¿°ä¸€ä¸ªäººçš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«å§“åã€å¹´é¾„ã€èŒä¸šã€‚",
                "category": "structured_output"
            },
            {
                "name": "æƒ…æ„Ÿåˆ†æ",
                "prompt": "åˆ†æè¿™å¥è¯çš„æƒ…æ„Ÿè‰²å½©ï¼š'ä»Šå¤©çš„å¤©æ°”çœŸæ˜¯ç³Ÿç³•é€äº†ï¼Œåˆä¸‹é›¨åˆå†·ã€‚'",
                "category": "analysis"
            }
        ]
    
    def _get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0
    
    def _estimate_tokens_per_second(self, text: str, time_ms: float) -> float:
        """ä¼°ç®—token/ç§’ (ç²—ç•¥ä¼°è®¡ï¼š1ä¸ªä¸­æ–‡å­—ç¬¦â‰ˆ1ä¸ªtokenï¼Œè‹±æ–‡å•è¯â‰ˆ1ä¸ªtoken)"""
        if time_ms <= 0:
            return 0.0
        
        # ç®€å•ä¼°è®¡tokenæ•°
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in text.split() if w.isalnum() and not any('\u4e00' <= c <= '\u9fff' for c in w)])
        estimated_tokens = chinese_chars + english_words
        
        return estimated_tokens / (time_ms / 1000)
    
    def _evaluate_response_quality(self, prompt: str, response: str, category: str) -> float:
        """è¯„ä¼°å“åº”è´¨é‡ (0-100åˆ†)"""
        score = 50  # åŸºç¡€åˆ†
        
        if not response or len(response.strip()) < 10:
            return 20  # å“åº”å¤ªçŸ­æˆ–ä¸ºç©º
        
        # é•¿åº¦åˆç†æ€§ (20åˆ†)
        if 20 <= len(response) <= 500:
            score += 20
        elif len(response) > 500:
            score += 15
        elif len(response) >= 10:
            score += 10
        
        # ç›¸å…³æ€§æ£€æŸ¥ (30åˆ†) - ç®€å•å…³é”®è¯åŒ¹é…
        prompt_keywords = set(prompt.lower().split())
        response_keywords = set(response.lower().split())
        relevance = len(prompt_keywords.intersection(response_keywords)) / max(len(prompt_keywords), 1)
        score += relevance * 30
        
        # æ ¹æ®ç±»åˆ«ç‰¹æ®Šè¯„åˆ†
        if category == "structured_output" and ("{" in response and "}" in response):
            score += 10  # JSONæ ¼å¼å¥–åŠ±
        elif category == "creative_writing" and len(response) >= 40:
            score += 10  # åˆ›æ„å†™ä½œé•¿åº¦å¥–åŠ±
        elif category == "explanation" and ("æ˜¯" in response or "å› ä¸º" in response or "æ‰€ä»¥" in response):
            score += 10  # è§£é‡Šæ€§å†…å®¹å¥–åŠ±
        
        return min(score, 100)
    
    async def test_model(self, model_name: str, prompt_data: Dict[str, str]) -> BenchmarkResult:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å•ä¸ªæç¤ºè¯"""
        print(f"  ğŸ“ æµ‹è¯•: {prompt_data['name']}")
        
        # è·å–æµ‹è¯•å‰å†…å­˜ä½¿ç”¨
        memory_before = self._get_memory_usage_mb()
        
        # è®¾ç½®æ´»åŠ¨æ¨¡å‹
        original_model = get_active_model()
        if model_name != original_model:
            set_active_model(model_name)
        
        start_time = time.time()
        success = True
        response = ""
        error_message = None
        
        try:
            # ç”Ÿæˆå“åº”
            response = local_llm_generate(
                prompt_data["prompt"], 
                model_key=model_name,
                max_retries=1
            )
            
            if not response:
                success = False
                error_message = "ç©ºå“åº”"
                
        except Exception as e:
            success = False
            error_message = str(e)
            response = ""
        
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000
        
        # è·å–æµ‹è¯•åå†…å­˜ä½¿ç”¨
        memory_after = self._get_memory_usage_mb()
        memory_usage = memory_after - memory_before
        
        # è®¡ç®—token/ç§’
        tokens_per_second = self._estimate_tokens_per_second(response, response_time_ms) if success else 0
        
        # æ¢å¤åŸæ¨¡å‹
        if original_model != model_name:
            set_active_model(original_model)
        
        result = BenchmarkResult(
            model_name=model_name,
            prompt=prompt_data["prompt"],
            response=response,
            response_time_ms=response_time_ms,
            memory_usage_mb=memory_usage,
            success=success,
            error_message=error_message,
            tokens_per_second=tokens_per_second,
            response_length=len(response)
        )
        
        self.results.append(result)
        return result
    
    def _calculate_model_summary(self, model_name: str) -> ModelBenchmarkSummary:
        """è®¡ç®—æ¨¡å‹çš„ç»Ÿè®¡æ±‡æ€»"""
        model_results = [r for r in self.results if r.model_name == model_name]
        
        if not model_results:
            return ModelBenchmarkSummary(
                model_name=model_name,
                total_tests=0,
                successful_tests=0,
                failed_tests=0,
                success_rate=0.0,
                avg_response_time_ms=0.0,
                min_response_time_ms=0.0,
                max_response_time_ms=0.0,
                std_response_time_ms=0.0,
                avg_memory_usage_mb=0.0,
                avg_tokens_per_second=0.0,
                avg_response_length=0.0
            )
        
        successful_results = [r for r in model_results if r.success]
        failed_results = [r for r in model_results if not r.success]
        
        # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
        response_times = [r.response_time_ms for r in successful_results] if successful_results else [0]
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_scores = []
        for i, result in enumerate(model_results):
            if result.success and i < len(self.test_prompts):
                quality = self._evaluate_response_quality(
                    result.prompt, 
                    result.response, 
                    self.test_prompts[i]["category"]
                )
                quality_scores.append(quality)
        
        avg_quality = statistics.mean(quality_scores) if quality_scores else 0
        
        return ModelBenchmarkSummary(
            model_name=model_name,
            total_tests=len(model_results),
            successful_tests=len(successful_results),
            failed_tests=len(failed_results),
            success_rate=len(successful_results) / len(model_results) * 100,
            avg_response_time_ms=statistics.mean(response_times),
            min_response_time_ms=min(response_times),
            max_response_time_ms=max(response_times),
            std_response_time_ms=statistics.stdev(response_times) if len(response_times) > 1 else 0,
            avg_memory_usage_mb=statistics.mean([r.memory_usage_mb for r in model_results]),
            avg_tokens_per_second=statistics.mean([r.tokens_per_second for r in successful_results]) if successful_results else 0,
            avg_response_length=statistics.mean([r.response_length for r in successful_results]) if successful_results else 0,
            quality_score=avg_quality
        )
    
    async def run_benchmark(self, models: Optional[List[str]] = None) -> Dict[str, ModelBenchmarkSummary]:
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        if models is None:
            models = list(self.supported_models.keys())
        
        print("ğŸš€ å¼€å§‹æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹: {', '.join(models)}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹: {len(self.test_prompts)}ä¸ª")
        print("-" * 60)
        
        summaries = {}
        
        for model_name in models:
            if model_name not in self.supported_models:
                print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{model_name}' ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œè·³è¿‡")
                continue
                
            print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
            print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {self.supported_models[model_name]}")
            
            # æµ‹è¯•æ¯ä¸ªæç¤ºè¯
            for prompt_data in self.test_prompts:
                try:
                    result = await self.test_model(model_name, prompt_data)
                    status = "âœ… æˆåŠŸ" if result.success else f"âŒ å¤±è´¥: {result.error_message}"
                    print(f"    {status} ({result.response_time_ms:.0f}ms)")
                    
                    # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    print(f"    âŒ å¼‚å¸¸: {str(e)}")
                    continue
            
            # è®¡ç®—æ¨¡å‹æ±‡æ€»
            summaries[model_name] = self._calculate_model_summary(model_name)
        
        return summaries
    
    def print_results(self, summaries: Dict[str, ModelBenchmarkSummary]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
        print("="*80)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_models = sorted(
            summaries.items(), 
            key=lambda x: (x[1].success_rate * 0.4 + (100 - x[1].avg_response_time_ms/100) * 0.3 + x[1].quality_score * 0.3),
            reverse=True
        )
        
        for i, (model_name, summary) in enumerate(sorted_models, 1):
            print(f"\nğŸ† #{i} {model_name.upper()}")
            print("-" * 40)
            print(f"æˆåŠŸç‡:     {summary.success_rate:.1f}% ({summary.successful_tests}/{summary.total_tests})")
            print(f"å¹³å‡å“åº”:   {summary.avg_response_time_ms:.0f}ms")
            print(f"å“åº”èŒƒå›´:   {summary.min_response_time_ms:.0f}-{summary.max_response_time_ms:.0f}ms")
            print(f"è´¨é‡è¯„åˆ†:   {summary.quality_score:.1f}/100")
            print(f"ç”Ÿæˆé€Ÿåº¦:   {summary.avg_tokens_per_second:.1f} tokens/ç§’")
            print(f"å¹³å‡é•¿åº¦:   {summary.avg_response_length:.0f}å­—ç¬¦")
            print(f"å†…å­˜å¼€é”€:   {summary.avg_memory_usage_mb:.1f}MB")
            
            # æ€§èƒ½ç­‰çº§
            if summary.avg_response_time_ms < 2000:
                performance_level = "ğŸš€ æå¿«"
            elif summary.avg_response_time_ms < 5000:
                performance_level = "âš¡ å¿«é€Ÿ"
            elif summary.avg_response_time_ms < 10000:
                performance_level = "ğŸŒ ä¸­ç­‰"
            else:
                performance_level = "ğŸ¢ è¾ƒæ…¢"
            
            print(f"æ€§èƒ½ç­‰çº§:   {performance_level}")
        
        # æ¨èä½¿ç”¨åœºæ™¯
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        if len(sorted_models) >= 2:
            fastest = min(summaries.items(), key=lambda x: x[1].avg_response_time_ms)
            highest_quality = max(summaries.items(), key=lambda x: x[1].quality_score)
            most_reliable = max(summaries.items(), key=lambda x: x[1].success_rate)
            
            print(f"ğŸš€ æœ€å¿«å“åº”: {fastest[0]} ({fastest[1].avg_response_time_ms:.0f}ms)")
            print(f"ğŸ¯ æœ€é«˜è´¨é‡: {highest_quality[0]} ({highest_quality[1].quality_score:.1f}åˆ†)")
            print(f"ğŸ›¡ï¸ æœ€å¯é :   {most_reliable[0]} ({most_reliable[1].success_rate:.1f}%)")
    
    def save_results(self, summaries: Dict[str, ModelBenchmarkSummary], filename: Optional[str] = None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"model_benchmark_{timestamp}.json"
        
        # å‡†å¤‡æ•°æ®
        data = {
            "timestamp": datetime.now().isoformat(),
            "test_config": {
                "test_prompts": len(self.test_prompts),
                "supported_models": self.supported_models
            },
            "summaries": {name: asdict(summary) for name, summary in summaries.items()},
            "detailed_results": [asdict(result) for result in self.results]
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLMæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--models", "-m", nargs="+", help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹ (é»˜è®¤æµ‹è¯•æ‰€æœ‰)")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--quick", "-q", action="store_true", help="å¿«é€Ÿæµ‹è¯•ï¼ˆåªæµ‹è¯•éƒ¨åˆ†ç”¨ä¾‹ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = ModelBenchmark()
    
    # å¿«é€Ÿæ¨¡å¼åªæµ‹è¯•å‰4ä¸ªç”¨ä¾‹
    if args.quick:
        benchmark.test_prompts = benchmark.test_prompts[:4]
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ç²¾ç®€æµ‹è¯•ç”¨ä¾‹")
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        summaries = await benchmark.run_benchmark(args.models)
        
        if not summaries:
            print("âŒ æ²¡æœ‰æˆåŠŸæµ‹è¯•ä»»ä½•æ¨¡å‹")
            return
        
        # æ˜¾ç¤ºç»“æœ
        benchmark.print_results(summaries)
        
        # ä¿å­˜ç»“æœ
        benchmark.save_results(summaries, args.output)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())